# -*- coding: utf-8 -*-
"""main.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1zNLxHjsZfijzRjZH4mjRb1oz5N1ObRL5
"""

import numpy as np
import numpy.linalg
import pandas as pd
import sklearn
import sklearn.preprocessing, sklearn.cluster, sklearn.metrics, sklearn.decomposition
import scipy.spatial
import matplotlib.pyplot as plt
import seaborn as sns

"""# SVD decomposition and dimension reduction

The following datasets contains 10000 vectorized images of the size 8x8 pixels with generated bars.

 - bars.csv - original data
 - bars_noise20.csv and bars_noise50.csv - the generated noise was distributed into original dataset
"""

X = pd.read_csv('bars.csv', header=None).values
X_noise_20 = pd.read_csv('bars_noise20.csv', header=None).values
X_noise_50 = pd.read_csv('bars_noise50.csv', header=None).values

X.shape

X[1]

X_noise_20[1]

def show_image(x):
    plt.imshow(x.reshape((8,8)), cmap='Greys')

show_image(X[2])

show_image(X_noise_20[2])

show_image(X_noise_50[2])

"""## SVD decompostioton"""

U, s, V_T = np.linalg.svd(X, full_matrices=True)
U.shape, s.shape, V_T.shape

s

"""What does the values in vector **s** represent?

### Reconstruct complete matrix and check error
"""

np.diag(s)

R = U[:, :64].dot(np.diag(s)).dot(V_T)
X-R

"""### Use only 5 main vectors (which represent reduction to 5 dimensions). Create reconstructed matrix and compare it with original one.
- https://csiu.github.io/blog//img/figure/2017-04-16/svd.png
![obrazek.png](attachment:0ff07704-dd0e-49bc-8839-322f279a3c6f.png)
"""

U[:, :5]

V_T[:5, :]

"""## Where can we find vector space in reduced dimension?"""

U[:, :5].shape

U[:, :5]

"""## Reconstruction from reduced space and error calculation"""

R = U[:,:5].dot(np.diag(s[:5])).dot(V_T[:5, :])
R.shape

show_image(X[2])
plt.title('Original image')

show_image(R[2])
plt.title('Reconstructed image when only 5 dimensions are used')

"""What is the error for reconstructed picture?"""

print(X[2]-R[2])

"""What is the error for reconstructed pictures?"""

print(X-R)

np.linalg.norm(X-R)

"""# Individual tasks

- **Objective of first task is to use reduction dimension to reduce noise in the synthetic datasets.**

- **Second task should demonstare usage of dimension reductions techniques for 2D visualization and showing clustering results in that vis.**

---

## Task 1(1p) : Complete following tasks for dimension reduction using SVD over our toy datasets (bars, bars_noise20, bars_noise50)

### Qustion: What does the number from previous cell mean? What is the output from *np.linalg.norm* function?

Answer:

### Find optimal number of features/dimension for our toy datasets
- Our datasets **X_noise_20** and **X_noise_50** contain noise, we want to use dimension reduction to eliminate impact of noise in our dataset
- **Try different number of dimensions for dimension reduction, calculate Frobenius norm for reconstructed and original matrix, create plot with reconstruction error values afterwards.** Is 5,10,15,20,...,40 dimensions enough if the original data had 64 dimensions?
- Repeat proces for datasets **X**, **X_noise_20**, **X_noise_50**
"""



"""### Interpret results based on previous graphs. What is the best number of dimensions? Why? Is there any connection between best dimension and process "how" the images were generated and what information they represent?

Interpretation:

### Can you "recover" information for following image?
"""

show_image(X_noise_50[2])

"""#### Does the dimension reduction have effect for noise reduction?
1. Visualize original image *X\[2\]* in form of 8x8 image.
2. Visualize appropriate image with with high noise *X_noise_50\[2\]*.
3. Perform dimension reduction for X_noise_50 and choose a best dimension.
4. Visualize reconstructed image (when only several dimensions were used) of image with noise. Did the use of reduced space helped?
"""

# prompt: #### Does the dimension reduction have effect for noise reduction?
# 1. Visualize original image *X\[2\]* in form of 8x8 image.
# 2. Visualize appropriate image with with high noise *X_noise_50\[2\]*.
# 3. Perform dimension reduction for X_noise_50 and choose a best dimension.
# 4. Visualize reconstructed image (when only several dimensions were used) of image with noise. Did the use of reduced space helped?

# 1. Visualize original image X[2]
show_image(X[2])
plt.title('Original Image')
plt.show()

# 2. Visualize image with high noise X_noise_50[2]
show_image(X_noise_50[2])
plt.title('Image with High Noise')
plt.show()

# 3. Perform dimension reduction for X_noise_50 and choose a best dimension
U, s, V_T = np.linalg.svd(X_noise_50, full_matrices=True)
errors = []
dimensions = range(5, 41, 5)
for num_dims in dimensions:
  R = U[:, :num_dims].dot(np.diag(s[:num_dims])).dot(V_T[:num_dims, :])
  error = np.linalg.norm(X_noise_50 - R)
  errors.append(error)

plt.plot(dimensions, errors)
plt.xlabel('Number of Dimensions')
plt.ylabel('Reconstruction Error')
plt.title('Reconstruction Error vs. Number of Dimensions (X_noise_50)')
plt.show()

# Choose the best dimension based on the plot (elbow point or where the error starts to level off)
best_num_dims = 15  # Example, you should choose based on the plot

# 4. Visualize reconstructed image
R = U[:, :best_num_dims].dot(np.diag(s[:best_num_dims])).dot(V_T[:best_num_dims, :])
show_image(R[2])
plt.title(f'Reconstructed Image with {best_num_dims} Dimensions')
plt.show()

"""## Task 2(1p) : The use of dimension reduction methods for visualization purposes

We have tried some clustering for **Red wine quality data** last week. I have copied the most important cells into this notebook.

1. Fill in any clustering method for our dataset **Red wine quality data**, you can use already scaled matrix **X_scaled**.
2. Lets use other dimension reduction techniques: PCA and TSNE from sklearn library and get 2D space representation for our matrix **X_scaled**.
3. Use 2D representation from dimension reduction methods for 2D visualization of **Red wine quality data** and use color for displaying clusters found in matrix **X_scaled** - create at least two scatter plots for data points and use cluster_id as point's color.Create multiple visualization based on reduction to two dimensions for raw and preprocessed data.

- https://scikit-learn.org/stable/modules/generated/sklearn.decomposition.PCA.html
- https://scikit-learn.org/stable/modules/generated/sklearn.manifold.TSNE.html
"""

df = pd.read_csv('winequality-red.csv')
df_only_numeric = df.select_dtypes(np.number).drop('quality', axis=1)
scaler = sklearn.preprocessing.StandardScaler()
X_scaled = scaler.fit_transform(df_only_numeric.values)

"""### Clustering

Use any clustering method for preprocessed data.

Be aware to apply clustering for **X_scaled**, which is n-diminensional data. Do not use clustering only on 2D representation after dimension reduction.
"""

# prompt: ## Task 2(1p) : The use of dimension reduction methods for visualization purposes
# We have tried some clustering for **Red wine quality data** last week. I have copied the most important cells into this notebook.
# 1. Fill in any clustering method for our dataset **Red wine quality data**, you can use already scaled matrix **X_scaled**.
# 2. Lets use other dimension reduction techniques: PCA and TSNE from sklearn library and get 2D space representation for our matrix **X_scaled**.
# 3. Use 2D representation from dimension reduction methods for 2D visualization of **Red wine quality data** and use color for displaying clusters found in matrix **X_scaled** - create at least two scatter plots for data points and use cluster_id as point's color.Create multiple visualization based on reduction to two dimensions for raw and preprocessed data.
# - https://scikit-learn.org/stable/modules/generated/sklearn.decomposition.PCA.html
# - https://scikit-learn.org/stable/modules/generated/sklearn.manifold.TSN

from sklearn.cluster import KMeans

# 1. Clustering (using KMeans as an example)
kmeans = KMeans(n_clusters=3, random_state=0)  # You can change the number of clusters
cluster_ids = kmeans.fit_predict(X_scaled)

# 2. Dimension Reduction (PCA and TSNE)
pca = sklearn.decomposition.PCA(n_components=2)
X_pca = pca.fit_transform(X_scaled)

tsne = sklearn.manifold.TSNE(n_components=2, random_state=0)
X_tsne = tsne.fit_transform(X_scaled)


# 3. Visualization
plt.figure(figsize=(12, 6))

plt.subplot(1, 2, 1)
plt.scatter(X_pca[:, 0], X_pca[:, 1], c=cluster_ids, cmap='viridis')
plt.title('PCA Visualization of Red Wine Data with Clusters')
plt.xlabel('Principal Component 1')
plt.ylabel('Principal Component 2')

plt.subplot(1, 2, 2)
plt.scatter(X_tsne[:, 0], X_tsne[:, 1], c=cluster_ids, cmap='viridis')
plt.title('t-SNE Visualization of Red Wine Data with Clusters')
plt.xlabel('t-SNE Component 1')
plt.ylabel('t-SNE Component 2')

plt.tight_layout()
plt.show()

# You can also create visualizations for raw data (df_only_numeric) using PCA and TSNE in a similar way.
# Just replace X_scaled with df_only_numeric.values in the PCA and TSNE steps.

"""### 2D vis

#### Should we rescale the data before using dimension reduction?
- I have copied some answers on previous question from discussions.

"In summary, use the correlation matrix R when within-variable range and scale widely differs, and use the covariance matrix C to preserve variance if the range and scale of variables is similar or in the same units of measure." https://stats.stackexchange.com/questions/53/pca-on-correlation-or-covariance

"There are multiple versions of principal component algorithms, but most select a first principal component that maximizes variance. That only makes sense if a one unit difference in each of your measures is equally important. Even if your measures are in the same units, say dollars, that may not be true. If your measures are in different units, say one in inches and one in dollars, it would only be by coincidence that one inch was equally important as one dollar." https://www.quora.com/Why-do-you-need-to-scale-your-data-before-applying-PCA

"If you have data in different scales, the dimensions with larger scale will dominate, this is probably not what you want." https://www.quora.com/Why-do-you-need-to-scale-your-data-before-applying-PCA

"Generally speaking, Normalization is needed before PCA. The key to the problem is the order of feature selection, and it's depends on the method of feature selection." https://stackoverflow.com/questions/46062679/right-order-of-doing-feature-selection-pca-and-normalization

"t-SNE is sensitive to feature-wise normalization; and no theory says that such normalization will in general improve or degrade results, it fully depends on your data and expectation. If you can make more sense with maps from un-normalized data, then it indicates that normalization is not good for your study." https://www.reddit.com/r/MachineLearning/comments/5ygh1q/d_data_preprocessing_tips_for_tsne/

"rescaling is necessary if you want the different dimensions to be treated with equal importance, since the 2-norm will be more heavily influenced by dimensions with large variance." https://stats.stackexchange.com/questions/164917/should-data-be-centeredscaled-before-applying-t-sne
"""

from sklearn.decomposition import PCA
from sklearn.manifold import TSNE



"""#### Sample output of 2D visualization obtained by PCA method with labels from k-Means clustering

![image-2.png](attachment:image-2.png)

#### Just a meme

![a4fcac8e595c3c020fde81e7ca1110b297e9abf2d4d06a93781397b6fc884b1f_1.jpg](attachment:a4fcac8e595c3c020fde81e7ca1110b297e9abf2d4d06a93781397b6fc884b1f_1.jpg)
"""